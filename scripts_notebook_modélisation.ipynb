{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "badfa893-369e-4a1c-aafe-1b9a17b58f79",
   "metadata": {},
   "source": [
    "# Sentiment Analysis : EDA and model testing\n",
    "\n",
    "Our project aims at deploying a sentiment prediction API\n",
    "\n",
    "In this notebook we will first conduct our EDA and then evaluate different approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "13c1f9d7-5b13-4acc-a3be-cfb5d433b76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# classic Librairies\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# NLTK imports\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "# sklearn import \n",
    "from sklearn import preprocessing\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "from sklearn import cluster\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# librairies for NLP\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "import gensim\n",
    "\n",
    "# Bert\n",
    "import transformers\n",
    "from transformers import TFAutoModel\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "d25ef6ee-dbd2-448b-a8ba-b5b72c9797aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#DATA DOC FROM DATASOURCE (Kaggle)\n",
    "\"\"\"\n",
    "\n",
    "    target: the polarity of the tweet (0 = negative, 2 = neutral, 4 = positive)\n",
    "    ids: The id of the tweet ( 2087)\n",
    "    date: the date of the tweet (Sat May 16 23:58:44 UTC 2009)\n",
    "    flag: The query (lyx). If there is no query, then this value is NO_QUERY.\n",
    "    user: the user that tweeted (robotickilldozr)\n",
    "    text: the text of the tweet (Lyx is cool)\n",
    "\n",
    "\"\"\"\n",
    "print()#used to not print documentation. Not pretty but effective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "9a2a0835-4d24-4996-ab8c-56c412a6368e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1600000, 6)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#have to specify header=None because columns names are not implemented correctly\n",
    "columns_names = [\"target\",\"id\",\"date\",\"flag\",\"user\",\"text\"]\n",
    "data = pd.read_csv(\"data/training.1600000.processed.noemoticon.csv\", header=None, encoding='latin-1',names=columns_names)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "b6a2fb82-fece-4c75-90ff-084c7748e7d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1600000 entries, 0 to 1599999\n",
      "Data columns (total 6 columns):\n",
      " #   Column  Non-Null Count    Dtype \n",
      "---  ------  --------------    ----- \n",
      " 0   target  1600000 non-null  int64 \n",
      " 1   id      1600000 non-null  int64 \n",
      " 2   date    1600000 non-null  object\n",
      " 3   flag    1600000 non-null  object\n",
      " 4   user    1600000 non-null  object\n",
      " 5   text    1600000 non-null  object\n",
      "dtypes: int64(2), object(4)\n",
      "memory usage: 73.2+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "428b19bc-fe8a-4683-8396-e3942f01e3ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>flag</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target          id                          date      flag  \\\n",
       "0       0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY   \n",
       "1       0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   \n",
       "2       0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY   \n",
       "3       0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "4       0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "\n",
       "              user                                               text  \n",
       "0  _TheSpecialOne_  @switchfoot http://twitpic.com/2y1zl - Awww, t...  \n",
       "1    scotthamilton  is upset that he can't update his Facebook by ...  \n",
       "2         mattycus  @Kenichan I dived many times for the ball. Man...  \n",
       "3          ElleCTF    my whole body feels itchy and like its on fire   \n",
       "4           Karoli  @nationwideclass no, it's not behaving at all....  "
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f538932-614a-4006-8467-efa8111ba336",
   "metadata": {},
   "source": [
    "Dataset seems clean \\\n",
    "Moving on to to cleaning of sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58035170-e355-47c6-8129-e763ee40837e",
   "metadata": {},
   "source": [
    "## Text pre_treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "15ecaa14-caa1-49c2-91eb-01a2c1134cd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"@switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D\""
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To understand what we are dealing with, let's see the first description\n",
    "desc = data.text[0]\n",
    "desc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a500d808-fbfe-4f29-8b40-98d91e11d027",
   "metadata": {},
   "source": [
    "The very first is an interessting case. It is an answer to a tweet including a picture. \\\n",
    "This should be ignored when analyzing tweet \\\n",
    "NOTE TO SELF : prepare a RegEx to ignore or clean out images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "606191e0-02e3-40f8-9fc2-335039e10c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a RegExp to tokenize only alphanumerics\n",
    "tokenizer = nltk.RegexpTokenizer(r'\\w+')\n",
    "list_of_words = tokenizer.tokenize(desc)\n",
    "list_of_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a2113c-a524-4f96-aaf1-995d72eb8dc5",
   "metadata": {},
   "source": [
    "This will not exclude images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "1533eda0-155c-4b34-982e-62e54cf721de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lower all characters\n",
    "lowered_list = [w.lower() for w in list_of_words]\n",
    "lowered_list\n",
    "#len(lowered_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6e221d-edd6-4aff-9b9e-721fb17aae48",
   "metadata": {},
   "source": [
    "Do we really want to lower_case all characters ? Emphasis can be derived by full caps comments, maybe it should be kept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "99f495eb-be84-4b6f-b45b-07e41fbcf1fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stopwords exploration\n",
    "len(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "# stopwords suppression\n",
    "def stop_word_filter(list_words) :\n",
    "    stop_w = nltk.corpus.stopwords.words('english')\n",
    "    filtered_w = [w for w in list_words if not w in stop_w]\n",
    "    filtered_w2 = [w for w in filtered_w if len(w) > 2]\n",
    "    return filtered_w2\n",
    "\n",
    "filtered_list = stop_word_filter(lowered_list)\n",
    "filtered_list\n",
    "#len(filtered_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ead11b0-3aef-4441-b941-40b3bb3690d0",
   "metadata": {},
   "source": [
    "We will use lemmatization here \\\n",
    "Tweet are already quite short, and stemming them would be faster but would lose us precious information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "d79f6106-09bc-4b2c-9379-a252f2d5ed56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmat(list_words) :\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lem_w = [lemmatizer.lemmatize(w) for w in list_words]\n",
    "    return lem_w\n",
    "\n",
    "lemmat_list = lemmat(filtered_list)\n",
    "#lemmat_list\n",
    "len(lemmat_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "68829a9c-f0ea-4517-ab56-93d489f30d93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'switchfoot http twitpic com 2y1zl awww bummer shoulda got david carr third day'"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transf_text = ' '.join(lemmat_list)\n",
    "transf_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "5866e0fc-6c06-4083-8e0b-19ebe4db56dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a treatment function that will be usable on the whole dataset\n",
    "\n",
    "def cleaning_description_lemmat(text) :\n",
    "    # Tools used reminder :\n",
    "    # tokenizer = nltk.RegexpTokenizer(r'\\w+')\n",
    "    # stop_w = nltk.corpus.stopwords.words('english')\n",
    "    # lemmatizer = WordNetLemmatizer()\n",
    "    # stem = PorterStemmer()\n",
    "    \n",
    "    list_of_words = tokenizer.tokenize(text)\n",
    "    lowered_list = [w.lower() for w in list_of_words]\n",
    "    filtered_list = stop_word_filter(lowered_list)\n",
    "    lemmat_list = lemmat(filtered_list)\n",
    "    transformed_text = ' '.join(lemmat_list)\n",
    "\n",
    "    return transformed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "77095790-9c7a-47c2-b2c5-7127069f96ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'happy charitytuesday thenspcc sparkscharity speakinguph4h'"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = data[\"text\"][1599999]\n",
    "test = cleaning_description_lemmat(text)\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "d28f627a-fdd7-4977-ab4c-074d31fa38fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kanam\\anaconda3\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:59: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "from joblib import Parallel, delayed\n",
    "\n",
    "# Function to process a chunk of data\n",
    "def process_chunk(chunk):\n",
    "    chunk['cleaned_text'] = chunk['text'].apply(cleaning_description_lemmat)\n",
    "    return chunk\n",
    "\n",
    "# Break your data into smaller chunks and process them in parallel\n",
    "data_chunks = np.array_split(data, 10)  # Split data into 10 chunks\n",
    "\n",
    "# Use parallel processing to clean the text\n",
    "results = Parallel(n_jobs=-1)(delayed(process_chunk)(chunk) for chunk in data_chunks)\n",
    "\n",
    "# Combine the results back into a single dataframe\n",
    "cleaned_data = pd.concat(results)\n",
    "#took 321 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368758a4-63ee-4f1e-9cb3-9e793bade2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data['cleaned_text'] = data['text'].apply(lambda x : cleaning_description_lemmat(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "f4a2cd6d-c019-4795-8771-5a77dd9f8a7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>flag</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "      <td>switchfoot http twitpic com 2y1zl awww bummer ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "      <td>upset update facebook texting might cry result...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "      <td>kenichan dived many time ball managed save res...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "      <td>whole body feel itchy like fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "      <td>nationwideclass behaving mad see</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target          id                          date      flag  \\\n",
       "0       0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY   \n",
       "1       0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   \n",
       "2       0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY   \n",
       "3       0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "4       0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "\n",
       "              user                                               text  \\\n",
       "0  _TheSpecialOne_  @switchfoot http://twitpic.com/2y1zl - Awww, t...   \n",
       "1    scotthamilton  is upset that he can't update his Facebook by ...   \n",
       "2         mattycus  @Kenichan I dived many times for the ball. Man...   \n",
       "3          ElleCTF    my whole body feels itchy and like its on fire    \n",
       "4           Karoli  @nationwideclass no, it's not behaving at all....   \n",
       "\n",
       "                                        cleaned_text  \n",
       "0  switchfoot http twitpic com 2y1zl awww bummer ...  \n",
       "1  upset update facebook texting might cry result...  \n",
       "2  kenichan dived many time ball managed save res...  \n",
       "3                    whole body feel itchy like fire  \n",
       "4                   nationwideclass behaving mad see  "
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d47a22-cea7-49a2-831a-3e0e87563335",
   "metadata": {},
   "source": [
    "## Bag Of Word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "2f777b29-0439-4aff-b0bd-7f06fc485a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How important is the bag of word ?\n",
    "\n",
    "#x = data['cleaned_text'].apply(lambda x : len(word_tokenize(x)))\n",
    "#print(\"max length bow : \", x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "64a0e93b-8b6c-4079-a590-eda0b62e6977",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create bag of words Tf-idf\n",
    "\n",
    "ctf = TfidfVectorizer(stop_words='english', max_df=0.95, min_df=1)\n",
    "\n",
    "ctf_fit = ctf.fit(cleaned_data['cleaned_text'])\n",
    " \n",
    "ctf_transform = ctf.transform(cleaned_data['cleaned_text'])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "8ba4d217-8005-45c6-9719-5f0217c93d27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1600000x670610 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 10272535 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ctf_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a25316d-2f15-4881-b0bb-53d9b15ec721",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA to reduce dimensions\n",
    "pca = PCA(svd_solver='arpack')\n",
    "pca.fit(ctf_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a7e0f6f-508a-4eb0-88c7-72e448fcc3c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = TSNE(init='random')\n",
    "\n",
    "TSNE_ctf_transform_2 = tsne.fit_transform(ctf_transform)\n",
    "TSNE_ctf_transform_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30cf4a58-246d-4b3f-aeda-3ad5c8330114",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment = cleaned_data['target']\n",
    "\n",
    "df_real_sentiment = pd.DataFrame({\n",
    "    'tsne_1': TSNE_ctf_transform_2[:, 0],\n",
    "    'tsne_2': TSNE_ctf_transform_2[:, 1],\n",
    "    'sentiment': sentiment\n",
    "})\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.scatterplot(\n",
    "    x='tsne_1', y='tsne_2', \n",
    "    hue='sentiment',\n",
    "    palette=sns.color_palette('hls', len(df_real_sentiment['sentiment'].unique())),\n",
    "    data=df_real_categories,\n",
    "    legend='full',\n",
    "    alpha=0.8\n",
    ")\n",
    "plt.title('2D t-SNE Representation Colored by General Category')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
